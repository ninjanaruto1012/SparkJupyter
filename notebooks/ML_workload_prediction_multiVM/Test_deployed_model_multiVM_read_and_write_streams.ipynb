{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acd2dc6",
   "metadata": {},
   "source": [
    "# Init Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ea826d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Warning: Ignoring non-Spark config property: executor.memory\n",
      "23/07/31 18:36:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from os.path import abspath\n",
    "import os\n",
    "\n",
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark-master:7077\"\n",
    "warehouse_location = './spark-warehouse'\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml-multiVM\")\n",
    "    .config(\"executor.memory\", \"8g\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    .config(\"spark.jars\", \"jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/kafka-clients-2.1.1.jar,jars/spark-streaming-kafka-0-10-assembly_2.12-3.2.1.jar,jars/commons-pool2-2.11.1.jar\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4868fb5",
   "metadata": {},
   "source": [
    "# We have 2 streams from 2 producers publishing the data on 2 topics. We will read 2 stream messages into sparks using spark streaming\n",
    "\n",
    "![Drag Racing](./images/kafka-spark-streaming2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebbde5",
   "metadata": {},
   "source": [
    "# Stream 1: Stream raw data of vm1 from kafka \n",
    "- Here we read the stream from kafka topic vm-stat-stream (acumos server) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3dd24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"vm-stat-stream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9da5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a62281",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringDF = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringDF.writeStream.format('console').start()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7c69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- cpu1: string (nullable = true)\n",
      " |-- memory1: string (nullable = true)\n",
      " |-- storage1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_vm1 = stringDF.withColumn('timestamp', regexp_extract('value', r'timestamp:\\s(.*),\\shostname', 1)) \\\n",
    "        .withColumn('cpu1', regexp_extract('value', r'used_cpu:\\s(.*)\\%', 1)) \\\n",
    "        .withColumn('memory1', regexp_extract('value', r'used_memory:\\s(.*)\\%,\\sused_storage', 1)) \\\n",
    "        .withColumn('storage1', regexp_extract('value', r'used_storage:\\s(.*)\\%,\\sused_cpu', 1))\n",
    "\n",
    "df_vm1 = df_vm1.drop('value')\n",
    "df_vm1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d1a0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm1_2 = df_vm1.withColumn(\n",
    "  'timestamp',\n",
    "  from_unixtime(unix_timestamp(\"timestamp\",\"dd-MM-yy hh:mm:ss a\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc021cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm1_2.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb32a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm1_water = df_vm1_2.withWatermark('timestamp','10 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae093a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/31 18:37:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ba85c8f3-3062-49b0-9fd4-b285d3c2e8f5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/07/31 18:37:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbcec437fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vm1_water.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedab3a",
   "metadata": {},
   "source": [
    "# Stream 2: Stream raw data of vm1 from kafka \n",
    "- Here we read the stream from kafka topic vm-stat-stream-2 (acumos server) into stringDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "414eb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "df2 = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"vm-stat-stream-2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd8096e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+-------+--------+\n",
      "|timestamp|cpu1|memory1|storage1|\n",
      "+---------+----+-------+--------+\n",
      "|     null|0.43|  38.86|      56|\n",
      "|     null|0.51|  38.85|      56|\n",
      "|     null|0.39|  38.98|      56|\n",
      "|     null|0.49|  38.98|      56|\n",
      "|     null|0.30|  38.98|      56|\n",
      "|     null|0.95|  38.98|      56|\n",
      "|     null|0.81|  38.99|      56|\n",
      "|     null|0.49|  38.99|      56|\n",
      "|     null|0.30|  38.99|      56|\n",
      "|     null|0.18|  38.86|      56|\n",
      "|     null|0.11|  39.00|      56|\n",
      "|     null|0.73|  38.99|      56|\n",
      "|     null|0.88|  38.99|      56|\n",
      "|     null|0.82|  39.00|      56|\n",
      "|     null|1.07|  38.99|      56|\n",
      "|     null|1.13|  39.01|      56|\n",
      "|     null|1.01|  39.00|      56|\n",
      "|     null|0.61|  38.87|      56|\n",
      "|     null|0.47|  38.88|      56|\n",
      "|     null|0.34|  38.87|      56|\n",
      "+---------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:24|2.79|  37.12|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:29|2.73|  37.67|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:34|2.83|  38.16|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:39|2.68|  38.23|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:45|2.71|  38.26|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:50|2.57|  38.27|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:37:55|2.45|  38.29|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:00|2.44|  38.32|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:06|2.33|  38.32|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:11|2.62|  38.32|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:16|2.49|  38.32|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:21|2.37|  38.37|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:26|2.18|  38.38|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringDF2 = df2.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9775623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:32|2.01|  38.37|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 15\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:37|1.85|  38.38|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 16\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:42|1.86|  38.36|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 17\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:47|1.79|  38.39|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 18\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:53|1.81|  38.39|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 19\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:38:58|1.66|  38.42|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 20\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:03|1.53|  38.43|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 21\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:08|1.65|  38.50|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 22\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:13|1.52|  38.50|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 23\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:19|1.39|  38.48|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 24\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:24|1.28|  38.48|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 25\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:29|1.18|  38.48|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 26\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:34|1.08|  38.48|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vm2 = stringDF2.withColumn('timestamp', regexp_extract('value', r'timestamp:\\s(.*),\\shostname', 1)) \\\n",
    "        .withColumn('cpu2', regexp_extract('value', r'used_cpu:\\s(.*)\\%', 1)) \\\n",
    "        .withColumn('memory2', regexp_extract('value', r'used_memory:\\s(.*)\\%,\\sused_storage', 1)) \\\n",
    "        .withColumn('storage2', regexp_extract('value', r'used_storage:\\s(.*)\\%,\\sused_cpu', 1))\n",
    "\n",
    "df_vm2 = df_vm2.drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a3beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm2_2 = df_vm2.withColumn(\n",
    "  'timestamp',\n",
    "  from_unixtime(unix_timestamp(\"timestamp\",\"dd-MM-yy hh:mm:ss a\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType())\n",
    ")\n",
    "df_vm2_2 = df_vm2_2.withColumnRenamed(\"timestamp\",\"timestamp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91933e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 27\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:40|1.08|  38.52|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vm2_water = df_vm2_2.withWatermark('timestamp2','10 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8895162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/31 18:39:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-824ad79b-bd18-4e3d-ab48-33b6aef492c3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/07/31 18:39:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbcdcde8e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----------+----+-------+--------+\n",
      "|timestamp2|cpu2|memory2|storage2|\n",
      "+----------+----+-------+--------+\n",
      "|      null|0.43|  38.86|      56|\n",
      "|      null|0.51|  38.85|      56|\n",
      "|      null|0.39|  38.98|      56|\n",
      "|      null|0.49|  38.98|      56|\n",
      "|      null|0.30|  38.98|      56|\n",
      "|      null|0.95|  38.98|      56|\n",
      "|      null|0.81|  38.99|      56|\n",
      "|      null|0.49|  38.99|      56|\n",
      "|      null|0.30|  38.99|      56|\n",
      "|      null|0.18|  38.86|      56|\n",
      "|      null|0.11|  39.00|      56|\n",
      "|      null|0.73|  38.99|      56|\n",
      "|      null|0.88|  38.99|      56|\n",
      "|      null|0.82|  39.00|      56|\n",
      "|      null|1.07|  38.99|      56|\n",
      "|      null|1.13|  39.01|      56|\n",
      "|      null|1.01|  39.00|      56|\n",
      "|      null|0.61|  38.87|      56|\n",
      "|      null|0.47|  38.88|      56|\n",
      "|      null|0.34|  38.87|      56|\n",
      "+----------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 28\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:45|1.39|  38.52|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:45|1.39|  38.52|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:50|1.28|  38.53|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 29\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:50|1.28|  38.53|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 30\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:55|1.18|  38.52|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:39:55|1.18|  38.52|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 31\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:01|1.22|  38.53|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:01|1.22|  38.53|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vm2_water.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c67cc",
   "metadata": {},
   "source": [
    "# Join two stream data into one stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be6d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 32\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:06|1.13|  38.55|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:06|1.13|  38.55|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join_water = df_vm1_water.join(df_vm2_water,expr(\"\"\"\n",
    "    timestamp = timestamp2 AND\n",
    "    timestamp2 >= timestamp AND\n",
    "    timestamp2 <= timestamp + interval 1 hour\n",
    "    \"\"\"),\"leftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2638f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_water.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c0b97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:>                 (0 + 1) / 1][Stage 40:>                 (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 33\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:11|1.04|  38.58|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:11|1.04|  38.58|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = df_vm1.join(df_vm2, 'timestamp' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b39a2d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/31 18:40:16 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f4390a75-52b8-4183-ada8-365b72caa84e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/07/31 18:40:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbcdcdd4610>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:>                 (0 + 1) / 1][Stage 42:>                 (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 34\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:16|0.95|  38.58|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:16|0.95|  38.58|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 43:>                 (0 + 1) / 1][Stage 44:>                 (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 35\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:21|0.88|  38.61|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:21|0.88|  38.61|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                       (0 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "df_join.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911a15b",
   "metadata": {},
   "source": [
    "# Publish joined stream data into topic 3 ('output-join-stat') in Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff8b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>(50 + 4) / 200][Stage 48:>   (0 + 0) / 1][Stage 49:>   (0 + 0) / 1]0]\r"
     ]
    }
   ],
   "source": [
    "nested_struct = struct(df_join.timestamp, df_join.cpu1, df_join.memory1, df_join.cpu2, df_join.memory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ac37d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:27|1.53|  39.23|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 36\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:27|1.53|  39.23|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_out = df_join.withColumn('value', to_json(nested_struct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52665098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 37\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:32|2.61|  39.62|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:32|2.61|  39.62|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 47:(102 + 2) / 200][Stage 50:>   (0 + 1) / 1][Stage 51:>   (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r",
      "23/07/31 18:40:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fbcec444460>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>(18 + 4) / 200][Stage 55:>   (0 + 0) / 1][Stage 56:>   (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------+--------+----+-------+--------+\n",
      "|           timestamp|cpu1|memory1|storage1|cpu2|memory2|storage2|\n",
      "+--------------------+----+-------+--------+----+-------+--------+\n",
      "|31-07-23 01:18:55 PM|0.40|  38.85|      59|0.40|  38.85|      59|\n",
      "|31-07-23 01:36:31 PM|5.86|  33.55|      60|5.86|  33.55|      60|\n",
      "|31-07-23 01:28:43 PM|7.75|  40.87|      60|7.75|  40.87|      60|\n",
      "|31-07-23 01:19:05 PM|0.50|  38.89|      59|0.50|  38.89|      59|\n",
      "|31-07-23 01:21:05 PM|2.25|  39.90|      59|2.25|  39.90|      59|\n",
      "|31-07-23 01:22:13 PM|5.17|  40.32|      59|5.17|  40.32|      59|\n",
      "|31-07-23 01:23:10 PM|5.73|  40.37|      59|5.73|  40.37|      59|\n",
      "|31-07-23 01:34:27 PM|7.40|  41.17|      60|7.40|  41.17|      60|\n",
      "|31-07-23 01:15:57 PM|0.71|  36.53|      59|0.71|  36.53|      59|\n",
      "|31-07-23 01:34:21 PM|7.70|  41.16|      60|7.70|  41.16|      60|\n",
      "|31-07-23 01:19:00 PM|0.36|  38.86|      59|0.36|  38.86|      59|\n",
      "|31-07-23 01:32:32 PM|7.91|  41.03|      60|7.91|  41.03|      60|\n",
      "|31-07-23 01:34:42 PM|7.07|  41.19|      60|7.07|  41.19|      60|\n",
      "|31-07-23 01:24:18 PM|5.69|  40.55|      59|5.69|  40.55|      59|\n",
      "|31-07-23 01:29:41 PM|6.69|  40.89|      60|6.69|  40.89|      60|\n",
      "|31-07-23 01:24:39 PM|5.61|  40.57|      59|5.61|  40.57|      59|\n",
      "|31-07-23 01:28:59 PM|7.74|  40.88|      60|7.74|  40.88|      60|\n",
      "|31-07-23 01:15:31 PM|0.96|  36.53|      59|0.96|  36.53|      59|\n",
      "|31-07-23 01:37:13 PM|3.30|  36.72|      60|3.30|  36.72|      60|\n",
      "|31-07-23 01:21:52 PM|4.39|  40.16|      59|4.39|  40.16|      59|\n",
      "+--------------------+----+-------+--------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 38\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:37|3.04|  39.80|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:37|3.04|  39.80|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 39\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:42|3.43|  40.06|      60|\n",
      "|2023-07-31 13:40:47|3.64|  40.15|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:42|3.43|  40.06|      60|\n",
      "|2023-07-31 13:40:47|3.64|  40.15|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 40\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:53|4.07|  40.26|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:53|4.07|  40.26|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|         timestamp2|cpu2|memory2|storage2|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:58|4.95|  40.43|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 41\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-31 13:40:58|4.95|  40.43|      60|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+----+-------+--------+----+-------+--------+\n",
      "|           timestamp|cpu1|memory1|storage1|cpu2|memory2|storage2|\n",
      "+--------------------+----+-------+--------+----+-------+--------+\n",
      "|31-07-23 01:40:32 PM|2.61|  39.62|      60|2.61|  39.62|      60|\n",
      "|31-07-23 01:40:37 PM|3.04|  39.80|      60|3.04|  39.80|      60|\n",
      "|31-07-23 01:40:27 PM|1.53|  39.23|      60|1.53|  39.23|      60|\n",
      "|31-07-23 01:40:21 PM|0.88|  38.61|      60|0.88|  38.61|      60|\n",
      "+--------------------+----+-------+--------+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>(96 + 4) / 200][Stage 69:>   (0 + 0) / 1][Stage 70:>   (0 + 0) / 1]0]\r"
     ]
    }
   ],
   "source": [
    "df_out.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"checkpointLocation\", \"./spark-warehouse/join-stream-kafka/checkpoint\") \\\n",
    "  .option(\"topic\", \"output-join-stat\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb01655",
   "metadata": {},
   "source": [
    "# Create consumer to read the joined DF in the topic 3 and make the predictions using latest stream data \n",
    "![Drag Racing](./images/kafka-predictions1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# import pyspark.pandas as ps\n",
    "# import pandas as pd\n",
    "\n",
    "# #convert spark dataframe to pandas for more visualization\n",
    "# n_vm = 2\n",
    "# df_dict={}\n",
    "# df_dict['vm1'] =  df.toPandas()\n",
    "# df_dict['vm2'] = df2.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename columns of two dataframe since now they have the same column names\n",
    "# for i in range(0,n_vm):\n",
    "#     df_dict['vm'+str(i+1)] = df_dict['vm'+str(i+1)].rename(columns={\"cpu\": \"cpu_vm\"+str(i+1), \"memory\": \"memory_vm\"+str(i+1),\"storage\": \"storage_vm\"+str(i+1)})\n",
    "#     df_dict['vm'+str(i+1)]['timestamp'] = pd.to_datetime(df_dict['vm'+str(i+1)]['timestamp'],format='%d-%m-%y %I:%M:%S %p').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     df_dict['vm'+str(i+1)]['timestamp']= pd.to_datetime(df_dict['vm'+str(i+1)]['timestamp'])\n",
    "#     df_dict['vm'+str(i+1)].set_index('timestamp',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c201ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join two time series using time stamp index union and sort the index of combined data frame according to time stamp\n",
    "# combined_df = df_dict['vm1'].join(df_dict['vm2'],how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.apply(pd.to_numeric, errors='ignore')\n",
    "# filled_df = combined_df.interpolate(method='ffill').interpolate(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad115d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=[]\n",
    "# for i in range(n_vm):\n",
    "#     cols.append('storage_vm'+str(i+1))\n",
    "# clean_df = filled_df.drop(columns=cols)\n",
    "# clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('total number of missing values in clean dataframe:',clean_df.isna().sum())\n",
    "# minute_df = clean_df.resample('1T').mean()\n",
    "# nan_count = minute_df.isna().sum()\n",
    "# print('total number of missing values in reampled dataframe:',nan_count)\n",
    "# minute_df = minute_df.fillna(method='ffill')\n",
    "# nan_count = minute_df.isna().sum()\n",
    "# print('total number of missing values in filled reampled dataframe:',nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = minute_df[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55decc23",
   "metadata": {},
   "source": [
    "# Make prediction\n",
    "- Registered model is ready deployed and the url to access the serve model is 'http://mlflowserve:5000/invocations'.\n",
    "- We construct a REST API call by using package requests of python to send the input X to retrieve the predicted y as follow\n",
    "\n",
    "In this example:\n",
    "- X must be an array which contains (n,input_steps,features) where number of features for the case of 2 VMs are 4\n",
    "- body data must be converted to json using json dumps with the fields 'inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# test_df_np = np.array(test_df)\n",
    "# test_input_np = np.expand_dims(test_df_np[0:30],axis=0)\n",
    "# print(test_input_np.shape)\n",
    "# test_input_list = test_input_np.tolist()\n",
    "# test_label_np = np.expand_dims(test_df_np[30:,[0,2]],axis=0)\n",
    "# print('test label shape:',test_label_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "\n",
    "# url = 'http://mlflowserve:5000/invocations'\n",
    "\n",
    "# headers = {'Content-Type': 'application/json'}\n",
    "# request_data = json.dumps({\"inputs\": test_input_list})\n",
    "# response = requests.post(url,request_data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_response = json.loads(response.content)\n",
    "# json_response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# max_subplots = 2\n",
    "# plot_col = 'cpu'\n",
    "# max_n = max_subplots\n",
    "# shift = 10\n",
    "# predictions = np.array(json_response['predictions'])\n",
    "# print(predictions.shape)\n",
    "# label_indices = np.arange(predictions.shape[1])\n",
    "# for n in range(max_n):\n",
    "#     plt.subplot(max_n, 1, n+1)\n",
    "#     plt.ylabel(f'{plot_col}')\n",
    "#     plt.plot(label_indices, test_label_np[0, :, n],\n",
    "#                 marker='^', label='Labels vm'+str(n+1))\n",
    "#     plt.plot(label_indices,  predictions[0, :, n],\n",
    "#                 label='prediction vm'+str(n+1), marker='x')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f3fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
