{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acd2dc6",
   "metadata": {},
   "source": [
    "# Init Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "17ea826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from os.path import abspath\n",
    "import os\n",
    "\n",
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark-master:7077\"\n",
    "warehouse_location = './spark-warehouse'\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml-multiVM\")\n",
    "    .config(\"executor.memory\", \"8g\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    .config(\"spark.jars\", \"jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/kafka-clients-2.1.1.jar,jars/spark-streaming-kafka-0-10-assembly_2.12-3.2.1.jar,jars/commons-pool2-2.11.1.jar\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebbde5",
   "metadata": {},
   "source": [
    "# Stream 1: Stream raw data of vm1 from kafka \n",
    "- Here we read the stream from kafka topic vm-stat-stream (acumos server) into stringDF and right away write this stream to a parquet format in the bronze domain at location ./spark-warehouse/stream_bronze_vm1\n",
    "- Then we transform the raw stream into cleaner version df_vm1 and write this stream to a parquet format in the silver domain at location  ./spark-warehouse/stream_silver_vm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fce01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"vm-stat-stream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b0784c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3323460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringDF = df1.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781eae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e14c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax to write stream to parquet\n",
    "\n",
    "# stringDF.writeStream.format(\"parquet\") \\\n",
    "#    .outputMode(\"append\") \\\n",
    "#    .option(\"checkpointLocation\", \"./spark-warehouse/stream_bronze_vm1/checkpoint\") \\\n",
    "#    .option(\"path\", \"/usr/bin/spark-3.2.4-bin-hadoop2.7/spark-warehouse/1/stream_bronze_vm1/data\") \\\n",
    "#    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb728668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- cpu1: string (nullable = true)\n",
      " |-- memory1: string (nullable = true)\n",
      " |-- storage1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_vm1 = stringDF.withColumn('timestamp', regexp_extract('value', r'timestamp:\\s(.*),\\sused_memory', 1)) \\\n",
    "        .withColumn('cpu1', regexp_extract('value', r'used_cpu:\\s(.*)\\%', 1)) \\\n",
    "        .withColumn('memory1', regexp_extract('value', r'used_memory:\\s(.*)\\%,\\sused_storage', 1)) \\\n",
    "        .withColumn('storage1', regexp_extract('value', r'used_storage:\\s(.*)\\%,\\sused_cpu', 1))\n",
    "\n",
    "df_vm1 = df_vm1.drop('value')\n",
    "df_vm1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed41378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vm1.writeStream.format(\"parquet\") \\\n",
    "#    .outputMode(\"append\") \\\n",
    "#    .option(\"checkpointLocation\", \"./spark-warehouse/stream_silver_vm1/checkpoint\") \\\n",
    "#    .option(\"path\", \"/usr/bin/spark-3.2.4-bin-hadoop2.7/spark-warehouse/1/stream_silver_vm1/data\") \\\n",
    "#    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedab3a",
   "metadata": {},
   "source": [
    "# Stream 2: Stream raw data of vm1 from kafka \n",
    "- Here we read the stream from kafka topic vm-stat-stream-2 (acumos server) into stringDF2 and right away write this stream to a parquet format in the bronze domain at location ./spark-warehouse/stream_bronze_vm2\n",
    "- Then we transform the raw stream into cleaner version df_vm2 and write this stream to a parquet format in the silver domain at location  ./spark-warehouse/stream_silver_vm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4fc873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"vm-stat-stream-2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aae3ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringDF2 = df2.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "957e59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax to write stream to parquet\n",
    "\n",
    "# stringDF2.writeStream.format(\"parquet\") \\\n",
    "#    .outputMode(\"append\") \\\n",
    "#    .option(\"checkpointLocation\", \"./spark-warehouse/stream_bronze_vm2/checkpoint\") \\\n",
    "#    .option(\"path\", \"/usr/bin/spark-3.2.4-bin-hadoop2.7/spark-warehouse/1/stream_bronze_vm2/data\") \\\n",
    "#    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5f4bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm2 = stringDF2.withColumn('timestamp', regexp_extract('value', r'timestamp:\\s(.*),\\sused_memory', 1)) \\\n",
    "        .withColumn('cpu2', regexp_extract('value', r'used_cpu:\\s(.*)\\%', 1)) \\\n",
    "        .withColumn('memory2', regexp_extract('value', r'used_memory:\\s(.*)\\%,\\sused_storage', 1)) \\\n",
    "        .withColumn('storage2', regexp_extract('value', r'used_storage:\\s(.*)\\%,\\sused_cpu', 1))\n",
    "\n",
    "df_vm2 = df_vm2.drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77b42181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vm2.writeStream.format(\"parquet\") \\\n",
    "#    .outputMode(\"append\") \\\n",
    "#    .option(\"checkpointLocation\", \"./spark-warehouse/stream_silver_vm2/checkpoint\") \\\n",
    "#    .option(\"path\", \"/usr/bin/spark-3.2.4-bin-hadoop2.7/spark-warehouse/1/stream_silver_vm2/data\") \\\n",
    "#    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "168a5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_vm1.join(df_vm2, 'timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "699f7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- cpu1: string (nullable = true)\n",
      " |-- memory1: string (nullable = true)\n",
      " |-- storage1: string (nullable = true)\n",
      " |-- cpu2: string (nullable = true)\n",
      " |-- memory2: string (nullable = true)\n",
      " |-- storage2: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/29 04:45:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "df_join.writeStream.format(\"parquet\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"checkpointLocation\", \"./spark-warehouse/join_stream/checkpoint\") \\\n",
    "   .option(\"path\", \"/usr/bin/spark-3.2.4-bin-hadoop2.7/spark-warehouse/3/join_stream/data\") \\\n",
    "   .start()\n",
    "\n",
    "df_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e818f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# path2 = './spark-warehouse/mondb.db/bronze_vmlog_maas_controller'\n",
    "# df2 = spark.read.format('parquet').options(header=True,inferSchema=True).load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46367188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "silverSchema = StructType([ \\\n",
    "    StructField(\"timestamp\",StringType(),True), \\\n",
    "    StructField(\"cpu1\",StringType(),True), \\\n",
    "    StructField(\"memory1\",StringType(),True), \\\n",
    "    StructField(\"storage1\", StringType(), True), \\\n",
    "    StructField(\"cpu2\",StringType(),True), \\\n",
    "    StructField(\"memory2\",StringType(),True), \\\n",
    "    StructField(\"storage2\", StringType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5c0742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.readStream.schema(silverSchema).load(\"/usr/bin/spark-3.2.4-bin-hadoop2.7/spark-warehouse/3/join_stream/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82e30603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f362710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/29 04:45:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7fa7fce5d9d0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.selectExpr(\"CAST(timestamp AS STRING)\", \"CAST(cpu1 AS STRING)\", \"CAST(memory1 AS STRING)\", \"CAST(storage1 AS STRING)\", \"CAST(cpu2 AS STRING)\", \"CAST(memory2 AS STRING)\", \"CAST(storage2 AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"topic\", \"output-join-\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d1857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560fdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52f31c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2917f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2070e2ac",
   "metadata": {},
   "source": [
    "# Join data of multiVM, fill missing values and clean the dataframe (drop storage features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cbb1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# import pyspark.pandas as ps\n",
    "# import pandas as pd\n",
    "\n",
    "# #convert spark dataframe to pandas for more visualization\n",
    "# n_vm = 2\n",
    "# df_dict={}\n",
    "# df_dict['vm1'] =  df.toPandas()\n",
    "# df_dict['vm2'] = df2.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf6f7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename columns of two dataframe since now they have the same column names\n",
    "# for i in range(0,n_vm):\n",
    "#     df_dict['vm'+str(i+1)] = df_dict['vm'+str(i+1)].rename(columns={\"cpu\": \"cpu_vm\"+str(i+1), \"memory\": \"memory_vm\"+str(i+1),\"storage\": \"storage_vm\"+str(i+1)})\n",
    "#     df_dict['vm'+str(i+1)]['timestamp'] = pd.to_datetime(df_dict['vm'+str(i+1)]['timestamp'],format='%d-%m-%y %I:%M:%S %p').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     df_dict['vm'+str(i+1)]['timestamp']= pd.to_datetime(df_dict['vm'+str(i+1)]['timestamp'])\n",
    "#     df_dict['vm'+str(i+1)].set_index('timestamp',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6c201ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/29 04:45:00 ERROR MicroBatchExecution: Query [id = b51fc1b4-4973-47ef-b829-64413c807941, runId = b47e4446-ffd5-4dbf-bcb3-d2a30b8ed535] terminated with error\n",
      "org.apache.spark.sql.AnalysisException: Required attribute 'value' not found\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWriter$.validateQuery(KafkaWriter.scala:59)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaStreamingWrite.<init>(KafkaStreamingWrite.scala:42)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaWrite.toStreaming(KafkaWrite.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.createStreamingWrite(StreamExecution.scala:607)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:140)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:59)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n"
     ]
    }
   ],
   "source": [
    "# join two time series using time stamp index union and sort the index of combined data frame according to time stamp\n",
    "# combined_df = df_dict['vm1'].join(df_dict['vm2'],how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2220baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3948b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.apply(pd.to_numeric, errors='ignore')\n",
    "# filled_df = combined_df.interpolate(method='ffill').interpolate(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad115d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=[]\n",
    "# for i in range(n_vm):\n",
    "#     cols.append('storage_vm'+str(i+1))\n",
    "# clean_df = filled_df.drop(columns=cols)\n",
    "# clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb7692",
   "metadata": {},
   "source": [
    "# Resample and aggregrate the data by time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1e3de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('total number of missing values in clean dataframe:',clean_df.isna().sum())\n",
    "# minute_df = clean_df.resample('1T').mean()\n",
    "# nan_count = minute_df.isna().sum()\n",
    "# print('total number of missing values in reampled dataframe:',nan_count)\n",
    "# minute_df = minute_df.fillna(method='ffill')\n",
    "# nan_count = minute_df.isna().sum()\n",
    "# print('total number of missing values in filled reampled dataframe:',nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "233d1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = minute_df[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55decc23",
   "metadata": {},
   "source": [
    "# Make prediction\n",
    "- Registered model is ready deployed and the url to access the serve model is 'http://mlflowserve:5000/invocations'.\n",
    "- We construct a REST API call by using package requests of python to send the input X to retrieve the predicted y as follow\n",
    "\n",
    "In this example:\n",
    "- X must be an array which contains (n,input_steps,features) where number of features for the case of 2 VMs are 4\n",
    "- body data must be converted to json using json dumps with the fields 'inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d94e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# test_df_np = np.array(test_df)\n",
    "# test_input_np = np.expand_dims(test_df_np[0:30],axis=0)\n",
    "# print(test_input_np.shape)\n",
    "# test_input_list = test_input_np.tolist()\n",
    "# test_label_np = np.expand_dims(test_df_np[30:,[0,2]],axis=0)\n",
    "# print('test label shape:',test_label_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7de9e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "\n",
    "# url = 'http://mlflowserve:5000/invocations'\n",
    "\n",
    "# headers = {'Content-Type': 'application/json'}\n",
    "# request_data = json.dumps({\"inputs\": test_input_list})\n",
    "# response = requests.post(url,request_data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad36b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_response = json.loads(response.content)\n",
    "# json_response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c34c8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 1.0 in stage 11.0 (TID 216) (172.24.0.11 executor 0): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 215) (172.24.0.10 executor 1): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/0/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=0),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/0/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/0/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/0/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 2.0 in stage 11.0 (TID 217) (172.24.0.11 executor 0): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/2/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=2),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/2/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/2/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/2/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 33.0 in stage 11.0 (TID 214) (172.24.0.10 executor 1): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/33/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=33),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/33/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/33/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/33/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 4.0 in stage 11.0 (TID 220) (172.24.0.11 executor 0): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/4/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=4),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/4/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/4/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/4/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 3.0 in stage 11.0 (TID 218) (172.24.0.11 executor 0): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/3/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=3),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/3/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/3/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/3/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "23/06/29 04:45:02 ERROR TaskSetManager: Task 1 in stage 11.0 failed 4 times; aborting job\n",
      "23/06/29 04:45:02 ERROR FileFormatWriter: Aborting job dab7c8d8-b416-44e1-a44a-77931da5b1c2.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 4 times, most recent failure: Lost task 1.3 in stage 11.0 (TID 229) (172.24.0.10 executor 1): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "23/06/29 04:45:02 ERROR MicroBatchExecution: Query [id = 708eec36-109d-470f-9447-8e57d12d5209, runId = 7935b97a-f236-467f-8ab7-d8a905d44f3f] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 4 times, most recent failure: Lost task 1.3 in stage 11.0 (TID 229) (172.24.0.10 executor 1): java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:228)\n",
      "\t... 25 more\n",
      "Caused by: java.lang.IllegalStateException: Error reading delta file file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta of HDFSStateStoreProvider[id = (op=0,part=1),dir = file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues]: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta does not exist\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:461)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$4(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat scala.runtime.java8.JFunction1$mcVJ$sp.apply(JFunction1$mcVJ$sp.java:23)\n",
      "\tat scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.$anonfun$loadMap$2(HDFSBackedStateStoreProvider.scala:416)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.loadMap(HDFSBackedStateStoreProvider.scala:388)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getLoadedMapForStore(HDFSBackedStateStoreProvider.scala:224)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:208)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:496)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$StateStoreHandler.getStateStore(SymmetricHashJoinStateManager.scala:414)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager$KeyToNumValuesStore.<init>(SymmetricHashJoinStateManager.scala:438)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager.<init>(SymmetricHashJoinStateManager.scala:382)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec$OneSideHashJoiner.<init>(StreamingSymmetricHashJoinExec.scala:508)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.processPartitions(StreamingSymmetricHashJoinExec.scala:257)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinExec.$anonfun$doExecute$1$adapted(StreamingSymmetricHashJoinExec.scala:228)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper$StateStoreAwareZipPartitionsRDD.compute(StreamingSymmetricHashJoinHelper.scala:235)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: file:/opt/workspace/ML_workload_prediction_multiVM/spark-warehouse/join_stream/checkpoint/state/0/1/left-keyToNumValues/1.delta\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:200)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.open(DelegateToFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.open(AbstractFileSystem.java:640)\n",
      "\tat org.apache.hadoop.fs.FilterFs.open(FilterFs.java:209)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:795)\n",
      "\tat org.apache.hadoop.fs.FileContext$6.next(FileContext.java:791)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.open(FileContext.java:797)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.open(CheckpointFileManager.scala:337)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:457)\n",
      "\t... 30 more\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 33.2 in stage 11.0 (TID 232) (172.24.0.10 executor 1): TaskKilled (Stage cancelled)\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 3.1 in stage 11.0 (TID 230) (172.24.0.11 executor 0): TaskKilled (Stage cancelled)\n",
      "23/06/29 04:45:02 WARN TaskSetManager: Lost task 2.3 in stage 11.0 (TID 231) (172.24.0.11 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# max_subplots = 2\n",
    "# plot_col = 'cpu'\n",
    "# max_n = max_subplots\n",
    "# shift = 10\n",
    "# predictions = np.array(json_response['predictions'])\n",
    "# print(predictions.shape)\n",
    "# label_indices = np.arange(predictions.shape[1])\n",
    "# for n in range(max_n):\n",
    "#     plt.subplot(max_n, 1, n+1)\n",
    "#     plt.ylabel(f'{plot_col}')\n",
    "#     plt.plot(label_indices, test_label_np[0, :, n],\n",
    "#                 marker='^', label='Labels vm'+str(n+1))\n",
    "#     plt.plot(label_indices,  predictions[0, :, n],\n",
    "#                 label='prediction vm'+str(n+1), marker='x')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f3fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
