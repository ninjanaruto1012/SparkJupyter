{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acd2dc6",
   "metadata": {},
   "source": [
    "# Init Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ea826d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Warning: Ignoring non-Spark config property: executor.memory\n",
      "23/07/28 20:41:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from os.path import abspath\n",
    "import os\n",
    "\n",
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark-master:7077\"\n",
    "warehouse_location = './spark-warehouse'\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml-multiVM\")\n",
    "    .config(\"executor.memory\", \"8g\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    .config(\"spark.jars\", \"jars/spark-sql-kafka-0-10_2.12-3.2.1.jar,jars/kafka-clients-2.1.1.jar,jars/spark-streaming-kafka-0-10-assembly_2.12-3.2.1.jar,jars/commons-pool2-2.11.1.jar\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4868fb5",
   "metadata": {},
   "source": [
    "# We have 2 streams from 2 producers publishing the data on 2 topics. We will read 2 stream messages into sparks using spark streaming\n",
    "\n",
    "![Drag Racing](./images/kafka-spark-streaming2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebbde5",
   "metadata": {},
   "source": [
    "# Stream 1: Stream raw data of vm1 from kafka \n",
    "- Here we read the stream from kafka topic vm-stat-stream (acumos server) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3dd24b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"onap-istanb-work01-stat-stream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9da5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a62281",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringDF = df1.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7c69a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- cpu1: string (nullable = true)\n",
      " |-- memory1: string (nullable = true)\n",
      " |-- storage1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_vm1 = stringDF.withColumn('timestamp', regexp_extract('value', r'timestamp:\\s(.*),\\shostname', 1)) \\\n",
    "        .withColumn('cpu1', regexp_extract('value', r'used_cpu:\\s(.*)\\%', 1)) \\\n",
    "#         .withColumn('memory1', regexp_extract('value', r'used_memory:\\s(.*)\\%,\\sused_storage', 1)) \\\n",
    "#         .withColumn('storage1', regexp_extract('value', r'used_storage:\\s(.*)\\%,\\sused_cpu', 1))\n",
    "\n",
    "df_vm1 = df_vm1.drop('value')\n",
    "df_vm1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1a0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm1_2 = df_vm1.withColumn(\n",
    "  'timestamp',\n",
    "  from_unixtime(unix_timestamp(\"timestamp\",\"dd-MM-yy hh:mm:ss a\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc021cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/28 20:47:22 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5c9d2615-e61d-417e-84b0-7ff8b93a5a6d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/07/28 20:47:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4bedf7ec10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-28 11:22:19|0.54|  25.53|      42|\n",
      "|2023-07-28 11:22:49|0.78|  25.53|      42|\n",
      "|2023-07-28 11:23:19|1.23|  25.53|      42|\n",
      "|2023-07-28 11:23:50|0.93|  25.53|      42|\n",
      "|2023-07-28 11:24:20|0.98|  25.53|      42|\n",
      "|2023-07-28 11:24:50|0.80|  25.53|      42|\n",
      "|2023-07-28 11:25:20|0.55|  25.53|      42|\n",
      "|2023-07-28 11:25:51|0.38|  25.54|      42|\n",
      "|2023-07-28 11:26:21|0.23|  25.54|      42|\n",
      "|2023-07-28 11:26:51|0.45|  25.59|      42|\n",
      "|2023-07-28 11:27:21|0.33|  25.53|      42|\n",
      "|2023-07-28 11:27:51|0.20|  25.54|      42|\n",
      "|2023-07-28 11:28:22|0.12|  25.53|      42|\n",
      "|2023-07-28 11:28:52|0.07|  25.53|      42|\n",
      "|2023-07-28 11:29:22|0.10|  25.52|      42|\n",
      "|2023-07-28 11:29:52|0.06|  25.53|      42|\n",
      "|2023-07-28 11:30:23|0.03|  25.54|      42|\n",
      "|2023-07-28 11:30:53|0.02|  25.54|      42|\n",
      "|2023-07-28 11:31:23|0.13|  25.52|      42|\n",
      "|2023-07-28 11:31:53|0.16|  25.53|      42|\n",
      "+-------------------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+----+-------+--------+\n",
      "|           timestamp|cpu1|memory1|storage1|\n",
      "+--------------------+----+-------+--------+\n",
      "|28-07-23 03:51:15...|0.14|  25.53|      42|\n",
      "+--------------------+----+-------+--------+\n",
      "\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-28 15:51:15|0.14|  25.53|      42|\n",
      "+-------------------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vm1_2.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb32a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm1_water = df_vm1_2.withWatermark('timestamp','10 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae093a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/28 20:47:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-54a0a525-a7bc-4d42-b96b-a302a338b44a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/07/28 20:47:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f4bedf7e340>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+----+-------+--------+\n",
      "|          timestamp|cpu1|memory1|storage1|\n",
      "+-------------------+----+-------+--------+\n",
      "|2023-07-28 11:22:19|0.54|  25.53|      42|\n",
      "|2023-07-28 11:22:49|0.78|  25.53|      42|\n",
      "|2023-07-28 11:23:19|1.23|  25.53|      42|\n",
      "|2023-07-28 11:23:50|0.93|  25.53|      42|\n",
      "|2023-07-28 11:24:20|0.98|  25.53|      42|\n",
      "|2023-07-28 11:24:50|0.80|  25.53|      42|\n",
      "|2023-07-28 11:25:20|0.55|  25.53|      42|\n",
      "|2023-07-28 11:25:51|0.38|  25.54|      42|\n",
      "|2023-07-28 11:26:21|0.23|  25.54|      42|\n",
      "|2023-07-28 11:26:51|0.45|  25.59|      42|\n",
      "|2023-07-28 11:27:21|0.33|  25.53|      42|\n",
      "|2023-07-28 11:27:51|0.20|  25.54|      42|\n",
      "|2023-07-28 11:28:22|0.12|  25.53|      42|\n",
      "|2023-07-28 11:28:52|0.07|  25.53|      42|\n",
      "|2023-07-28 11:29:22|0.10|  25.52|      42|\n",
      "|2023-07-28 11:29:52|0.06|  25.53|      42|\n",
      "|2023-07-28 11:30:23|0.03|  25.54|      42|\n",
      "|2023-07-28 11:30:53|0.02|  25.54|      42|\n",
      "|2023-07-28 11:31:23|0.13|  25.52|      42|\n",
      "|2023-07-28 11:31:53|0.16|  25.53|      42|\n",
      "+-------------------+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vm1_water.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cedab3a",
   "metadata": {},
   "source": [
    "# Stream 2: Stream raw data of vm1 from kafka \n",
    "- Here we read the stream from kafka topic vm-stat-stream-2 (acumos server) into stringDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414eb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"vm-stat-stream-2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringDF2 = df2.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9775623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm2 = stringDF2.withColumn('timestamp', regexp_extract('value', r'timestamp:\\s(.*),\\sused_memory', 1)) \\\n",
    "        .withColumn('cpu2', regexp_extract('value', r'used_cpu:\\s(.*)\\%', 1)) \\\n",
    "        .withColumn('memory2', regexp_extract('value', r'used_memory:\\s(.*)\\%,\\sused_storage', 1)) \\\n",
    "        .withColumn('storage2', regexp_extract('value', r'used_storage:\\s(.*)\\%,\\sused_cpu', 1))\n",
    "\n",
    "df_vm2 = df_vm2.drop('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a3beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm2_2 = df_vm2.withColumn(\n",
    "  'timestamp',\n",
    "  from_unixtime(unix_timestamp(\"timestamp\",\"dd-MM-yy hh:mm:ss a\"),\"yyyy-MM-dd HH:mm:ss\").cast(TimestampType())\n",
    ")\n",
    "df_vm2_2 = df_vm2_2.withColumnRenamed(\"timestamp\",\"timestamp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91933e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm2_water = df_vm2_2.withWatermark('timestamp2','10 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vm2_water.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c67cc",
   "metadata": {},
   "source": [
    "# Join two stream data into one stream "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_water = df_vm1_water.join(df_vm2_water,expr(\"\"\"\n",
    "    timestamp = timestamp2 AND\n",
    "    timestamp2 >= timestamp AND\n",
    "    timestamp2 <= timestamp + interval 1 hour\n",
    "    \"\"\"),\"leftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2638f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_water.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_vm1.join(df_vm2, 'timestamp' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911a15b",
   "metadata": {},
   "source": [
    "# Publish joined stream data into topic 3 ('output-join-stat') in Kafka broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_struct = struct(df_join.timestamp, df_join.cpu1, df_join.memory1, df_join.cpu2, df_join.memory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_join.withColumn('value', to_json(nested_struct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52665098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"checkpointLocation\", \"./spark-warehouse/join-stream-kafka/checkpoint\") \\\n",
    "  .option(\"topic\", \"output-join-stat\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb01655",
   "metadata": {},
   "source": [
    "# Create consumer to read the joined DF in the topic 3 and make the predictions using latest stream data \n",
    "![Drag Racing](./images/kafka-predictions1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# import pyspark.pandas as ps\n",
    "# import pandas as pd\n",
    "\n",
    "# #convert spark dataframe to pandas for more visualization\n",
    "# n_vm = 2\n",
    "# df_dict={}\n",
    "# df_dict['vm1'] =  df.toPandas()\n",
    "# df_dict['vm2'] = df2.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename columns of two dataframe since now they have the same column names\n",
    "# for i in range(0,n_vm):\n",
    "#     df_dict['vm'+str(i+1)] = df_dict['vm'+str(i+1)].rename(columns={\"cpu\": \"cpu_vm\"+str(i+1), \"memory\": \"memory_vm\"+str(i+1),\"storage\": \"storage_vm\"+str(i+1)})\n",
    "#     df_dict['vm'+str(i+1)]['timestamp'] = pd.to_datetime(df_dict['vm'+str(i+1)]['timestamp'],format='%d-%m-%y %I:%M:%S %p').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     df_dict['vm'+str(i+1)]['timestamp']= pd.to_datetime(df_dict['vm'+str(i+1)]['timestamp'])\n",
    "#     df_dict['vm'+str(i+1)].set_index('timestamp',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c201ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join two time series using time stamp index union and sort the index of combined data frame according to time stamp\n",
    "# combined_df = df_dict['vm1'].join(df_dict['vm2'],how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df = combined_df.apply(pd.to_numeric, errors='ignore')\n",
    "# filled_df = combined_df.interpolate(method='ffill').interpolate(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad115d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=[]\n",
    "# for i in range(n_vm):\n",
    "#     cols.append('storage_vm'+str(i+1))\n",
    "# clean_df = filled_df.drop(columns=cols)\n",
    "# clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('total number of missing values in clean dataframe:',clean_df.isna().sum())\n",
    "# minute_df = clean_df.resample('1T').mean()\n",
    "# nan_count = minute_df.isna().sum()\n",
    "# print('total number of missing values in reampled dataframe:',nan_count)\n",
    "# minute_df = minute_df.fillna(method='ffill')\n",
    "# nan_count = minute_df.isna().sum()\n",
    "# print('total number of missing values in filled reampled dataframe:',nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233d1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = minute_df[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55decc23",
   "metadata": {},
   "source": [
    "# Make prediction\n",
    "- Registered model is ready deployed and the url to access the serve model is 'http://mlflowserve:5000/invocations'.\n",
    "- We construct a REST API call by using package requests of python to send the input X to retrieve the predicted y as follow\n",
    "\n",
    "In this example:\n",
    "- X must be an array which contains (n,input_steps,features) where number of features for the case of 2 VMs are 4\n",
    "- body data must be converted to json using json dumps with the fields 'inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# test_df_np = np.array(test_df)\n",
    "# test_input_np = np.expand_dims(test_df_np[0:30],axis=0)\n",
    "# print(test_input_np.shape)\n",
    "# test_input_list = test_input_np.tolist()\n",
    "# test_label_np = np.expand_dims(test_df_np[30:,[0,2]],axis=0)\n",
    "# print('test label shape:',test_label_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import requests\n",
    "\n",
    "# url = 'http://mlflowserve:5000/invocations'\n",
    "\n",
    "# headers = {'Content-Type': 'application/json'}\n",
    "# request_data = json.dumps({\"inputs\": test_input_list})\n",
    "# response = requests.post(url,request_data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_response = json.loads(response.content)\n",
    "# json_response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# max_subplots = 2\n",
    "# plot_col = 'cpu'\n",
    "# max_n = max_subplots\n",
    "# shift = 10\n",
    "# predictions = np.array(json_response['predictions'])\n",
    "# print(predictions.shape)\n",
    "# label_indices = np.arange(predictions.shape[1])\n",
    "# for n in range(max_n):\n",
    "#     plt.subplot(max_n, 1, n+1)\n",
    "#     plt.ylabel(f'{plot_col}')\n",
    "#     plt.plot(label_indices, test_label_np[0, :, n],\n",
    "#                 marker='^', label='Labels vm'+str(n+1))\n",
    "#     plt.plot(label_indices,  predictions[0, :, n],\n",
    "#                 label='prediction vm'+str(n+1), marker='x')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f3fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
