{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c38f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: executor.memory\n",
      "23/05/31 03:13:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark-master:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark-ml\")\n",
    "    .config(\"executor.memory\", \"8g\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d0625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|_c0|      Date|AveragePrice|Total Volume|   4046|     4225| 4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|region|\n",
      "+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "|  0|2015-12-27|        1.33|    64236.62|1036.74| 54454.85|48.16|   8696.87|   8603.62|     93.25|        0.0|conventional|2015|Albany|\n",
      "|  1|2015-12-20|        1.35|    54876.98| 674.28| 44638.81|58.33|   9505.56|   9408.07|     97.49|        0.0|conventional|2015|Albany|\n",
      "|  2|2015-12-13|        0.93|   118220.22|  794.7|109149.67|130.5|   8145.35|   8042.21|    103.14|        0.0|conventional|2015|Albany|\n",
      "|  3|2015-12-06|        1.08|    78992.15| 1132.0| 71976.41|72.58|   5811.16|    5677.4|    133.76|        0.0|conventional|2015|Albany|\n",
      "+---+----------+------------+------------+-------+---------+-----+----------+----------+----------+-----------+------------+----+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_avocado = spark.read.csv(\n",
    "  \"avocado.csv\", \n",
    "  header=True, \n",
    "  inferSchema=True\n",
    ")\n",
    "\n",
    "# cache data\n",
    "df_avocado.cache()\n",
    "df_avocado.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e51dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avocado_train, df_avocado_test = df_avocado.randomSplit([0.75, 0.25], seed=214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337786be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+----------------+\n",
      "|_c0|      Date|AveragePrice|Total Volume|     4046|     4225|    4770|Total Bags|Small Bags|Large Bags|XLarge Bags|        type|year|          region|\n",
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+----------------+\n",
      "|  0|2015-12-27|        0.49|  1137707.43| 738314.8|286858.37|11642.46|  100891.8|  70749.02|  30142.78|        0.0|conventional|2015|   PhoenixTucson|\n",
      "|  0|2015-12-27|        0.71|   776404.39|451904.51|141599.36|15486.97| 167413.55| 123158.22|  33065.33|    11190.0|conventional|2015|WestTexNewMexico|\n",
      "+---+----------+------------+------------+---------+---------+--------+----------+----------+----------+-----------+------------+----+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_avocado_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c54841d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75182f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "|AveragePrice|        type|          LOG 4225|          LOG 4770|    LOG Small Bags|    LOG Large Bags|  LOG XLarge Bags|year|month|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "|        0.49|conventional|12.566747374652527| 9.362499927974252|11.166908098190957|10.313733879047971|              0.0|  15|   12|\n",
      "|        0.71|conventional|11.860764002611406| 9.647818872531012| 11.72123326879331| 10.40627082310141|9.322865162818028|  15|   12|\n",
      "|         0.8|conventional| 12.53017497505446|11.349393905288467|11.824526973139381| 9.415621332905047|9.658771095406955|  15|   12|\n",
      "|         0.8|conventional|13.028501871764691|11.364461534887267|13.490872079413348| 11.21667384527801|9.342104328605496|  15|   12|\n",
      "+------------+------------+------------------+------------------+------------------+------------------+-----------------+----+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "import numpy\n",
    "\n",
    "COLUMNS = ['AveragePrice', 'type']\n",
    "COLUMNS = [f\"`{col}`\" for col in COLUMNS]\n",
    "\n",
    "LOG_COLUMNS =  ['4225', '4770', 'Small Bags', 'Large Bags', 'XLarge Bags']\n",
    "LOG_COLUMNS = [f\"LOG(`{col}`+1) AS `LOG {col}`\" for col in LOG_COLUMNS]\n",
    "\n",
    "sql_trans = SQLTransformer(\n",
    "    statement=f\"\"\"\n",
    "    SELECT\n",
    "    {', '.join(COLUMNS)}\n",
    "    , {', '.join(LOG_COLUMNS)}\n",
    "    ,YEAR(__THIS__.Date)-2000 AS year\n",
    "    ,MONTH(__THIS__.Date) AS month\n",
    "\n",
    "    FROM __THIS__\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "sql_trans.transform(df_avocado_train).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd4c572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+\n",
      "|month|month_vec|month_scaled|\n",
      "+-----+---------+------------+\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "|   12|   [12.0]|       [1.0]|\n",
      "+-----+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# Creating a Month vector column\n",
    "month_vec_ass = VectorAssembler(inputCols=['month'], outputCol='month_vec')\n",
    "\n",
    "df_avocado_month_ass = month_vec_ass.transform(sql_trans.transform(df_avocado_train))\n",
    "\n",
    "# Scaling the month column\n",
    "month_scaler = MinMaxScaler(inputCol='month_vec', outputCol='month_scaled')\n",
    "month_scaler = month_scaler.fit(df_avocado_month_ass)\n",
    "\n",
    "month_scaler\\\n",
    "  .transform(df_avocado_month_ass)\\\n",
    "  .select( ['month', 'month_vec', 'month_scaled'] )\\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09459155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|        type|type_index|\n",
      "+------------+----------+\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "|conventional|       0.0|\n",
      "+------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "str_indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "\n",
    "str_indexer = str_indexer.fit(df_avocado_train)\n",
    "\n",
    "str_indexer\\\n",
    "  .transform(df_avocado_train)\\\n",
    "  .select( [\"type\", \"type_index\"] )\\\n",
    "  .show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b650e4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------------------------------------------------------------+------------+\n",
      "|features_cat|features_num                                                                                           |AveragePrice|\n",
      "+------------+-------------------------------------------------------------------------------------------------------+------------+\n",
      "|[0.0]       |[15.0,1.0,12.566747374652527,9.362499927974252,11.166908098190957,10.313733879047971,0.0]              |0.49        |\n",
      "|[0.0]       |[15.0,1.0,11.860764002611406,9.647818872531012,11.72123326879331,10.40627082310141,9.322865162818028]  |0.71        |\n",
      "|[0.0]       |[15.0,1.0,12.53017497505446,11.349393905288467,11.824526973139381,9.415621332905047,9.658771095406955] |0.8         |\n",
      "|[0.0]       |[15.0,1.0,13.028501871764691,11.364461534887267,13.490872079413348,11.21667384527801,9.342104328605496]|0.8         |\n",
      "+------------+-------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations\n",
    "## SQL transformer\n",
    "df_avocado_train_transformed = sql_trans.transform(df_avocado_train)\n",
    "\n",
    "## String indexer\n",
    "df_avocado_train_transformed = str_indexer.transform(df_avocado_train_transformed)\n",
    "\n",
    "## Month scaler (vector assembler + minmax scaler)\n",
    "df_avocado_train_transformed = month_vec_ass.transform(df_avocado_train_transformed)\n",
    "df_avocado_train_transformed = month_scaler.transform(df_avocado_train_transformed)\n",
    "\n",
    "\n",
    "# Join all features into a single vector\n",
    "numerical_vec_ass = VectorAssembler(\n",
    "    inputCols=[\n",
    "      'year', 'month_scaled', 'LOG 4225', \n",
    "      'LOG 4770', 'LOG Small Bags', \n",
    "      'LOG Large Bags', 'LOG XLarge Bags'\n",
    "    ],\n",
    "    outputCol='features_num'\n",
    ")\n",
    "df_avocado_train_transformed = numerical_vec_ass.transform(df_avocado_train_transformed)\n",
    "\n",
    "# Join all categorical features into a single vector\n",
    "categorical_vec_ass = VectorAssembler(\n",
    "    inputCols=['type_index'],\n",
    "    outputCol='features_cat'\n",
    ")\n",
    "df_avocado_train_transformed = categorical_vec_ass.transform(df_avocado_train_transformed)\n",
    "\n",
    "\n",
    "# See the result\n",
    "df_avocado_train_transformed.select(['features_cat', 'features_num', 'AveragePrice']).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4651358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_scaled                                                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295]|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976]  |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594] |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053] |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.4787880607250015,1.8713767178927097,1.4321533934378963,1.1582533794554424,2.5870627060190463] |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Scaling the numerical features using a StandardScaler\n",
    "std_scaler = StandardScaler(\n",
    "    inputCol=\"features_num\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "std_scaler = std_scaler.fit(df_avocado_train_transformed)\n",
    "std_scaler.transform(df_avocado_train_transformed).select(['features_scaled']).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89cbd60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|features                                                                                                                                    |AveragePrice|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine learning pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "prepro_pipe = Pipeline(stages=[\n",
    "    sql_trans,\n",
    "    str_indexer,\n",
    "    month_vec_ass,\n",
    "    month_scaler,\n",
    "    numerical_vec_ass,\n",
    "    categorical_vec_ass,\n",
    "    std_scaler,\n",
    "\n",
    "    # Join all features into a single vector\n",
    "    VectorAssembler(\n",
    "        inputCols=['features_scaled', 'features_cat'],\n",
    "        outputCol='features'\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline_model = prepro_pipe.fit(df_avocado_train)\n",
    "\n",
    "# Transform the data\n",
    "df_avocado_train_transformed = pipeline_model.transform(df_avocado_train)\n",
    "\n",
    "# See the result\n",
    "df_avocado_train_transformed.select(['features', 'AveragePrice']).show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f619eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create a linear regression model\n",
    "lin_reg = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "\n",
    "    # Hyperaparameters\n",
    "    maxIter=1000,\n",
    "    regParam=0.3,       # Regularization\n",
    "    elasticNetParam=0.8 # Regularization mixing parameter. 1 for L1, 0 for L2.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0dea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/31 03:16:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/31 03:16:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|features                                                                                                                                    |AveragePrice|prediction        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9527463109714546,1.0269649008115518,0.5657377199959452,0.8334134211814762,-0.6436162273445295,0.0]|0.49        |1.4003505112793717|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.7058305701685025,1.0954357394643428,0.7803295242390127,0.8574417380503548,2.012648481596976,0.0]  |0.71        |1.4003505112793717|\n",
      "|[-1.2177154955881637,1.6482225355667333,0.9399552148956506,1.5037797059140563,0.8203168521795554,0.6002078289352569,2.1083545825302594,0.0] |0.8         |1.4003505112793717|\n",
      "|[-1.2177154955881637,1.6482225355667333,1.1142436751287843,1.5073956355774096,1.4653967110976907,1.0678725104034048,2.0181300922626053,0.0] |0.8         |1.4003505112793717|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "lin_reg_model = lin_reg.fit(df_avocado_train_transformed)\n",
    "\n",
    "# See the output\n",
    "df_avocado_train_pred = lin_reg_model.transform(df_avocado_train_transformed)\n",
    "df_avocado_train_pred.select(\n",
    "  ['features', 'AveragePrice', 'prediction']\n",
    ").show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd8a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_reg_model.save('mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a152e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "# load_model = LinearRegressionModel.load('mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c52e44e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3978489578943717"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "reg_eval = RegressionEvaluator(\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse' # Root mean squared error\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "reg_eval.evaluate(df_avocado_train_pred)\n",
    "\n",
    "# Output >> 0.3978489578943717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "\n",
    "ml_pipeline = Pipeline(stages=[\n",
    "    prepro_pipe, # Preprocessing pipeline\n",
    "    lin_reg      # Linear regression model\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lin_reg.regParam, [0.0, 0.1, 0.3, 0.5]) \\\n",
    "    .addGrid(lin_reg.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483bb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_eval = RegressionEvaluator(\n",
    "    labelCol='AveragePrice',\n",
    "    predictionCol='prediction',\n",
    "    metricName='rmse' # Root mean squared error\n",
    ")\n",
    "\n",
    "# Join everything together using a CrossValidator object.\n",
    "crossval_ml = CrossValidator(\n",
    "    estimator=ml_pipeline, \n",
    "    estimatorParamMaps=param_grid, \n",
    "    evaluator=reg_eval, \n",
    "    numFolds=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d610b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_ml_model = crossval_ml.fit(df_avocado_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999264c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
